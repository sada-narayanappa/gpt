{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT From scratch\n",
    "\n",
    "Look upand read paper on Attention is all you need.\n",
    "https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "## Acknowledgements:\n",
    "The contents have been organized from these references:\n",
    "\n",
    "1. evshahs.medium.com/build-gpt-with-me-implementing-gpt-from-scratch-step-by-step-b2efe4e2f7e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuzzPHd6gIUQ",
    "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"Comment this line to run this cell\"\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EsxxG36STID"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load the text\n",
    "2. Create a map of character to numeric representation of some sort\n",
    "\n",
    "'''\n",
    "text = open(\"input.txt\").read()\n",
    "chars = sorted(list(set(text))) #get all the characters in the first 1000 characters\n",
    "vocab_size = len(chars) # get the size of it\n",
    "char_to_index = {char: index for index, char in enumerate(chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(chars)}\n",
    "\n",
    "print(f\"vocab_size: {vocab_size}, Sample chars: {chars[0:10]}\") #, char_to_index, index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a sentence, convert to a vector \n",
    "def encode_string(s):\n",
    "    encoded_list = [char_to_index[char] for char in s]\n",
    "    return encoded_list\n",
    "\n",
    "def decode_list(l):\n",
    "    decoded_string = ''.join([index_to_char[index] for index in l])\n",
    "    return decoded_string\n",
    "\n",
    "#Example \n",
    "enc = encode_string(\"babu\")\n",
    "dec = decode_list(enc)\n",
    "print(f\"Encoded: {enc} ==> decoded: '{dec}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode_string(text), dtype=torch.long)\n",
    "assert len(data) == len(text), \"Hmmm whats Wrong\"\n",
    "len(data), len(text), data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to test the function\n",
    "batch_size  = 4\n",
    "block_size  = 10\n",
    "train       = data[0: int(.9*len(data)) ]  # 90% training, remaining validations\n",
    "val_data    = data[len(train):]\n",
    "\n",
    "# Create some batch of data: get some sample data of batch size\n",
    "# \n",
    "# It works as follows:\n",
    "# 1. pick random positions in the data\n",
    "# 2. create array of (batch_size X block_size) \n",
    "# 3. Block_size is the window length or context length\n",
    "#  \n",
    "def get_batch(split=\"train\"):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # get a random value\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # the first block size (context)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # the target\n",
    "    return x, y\n",
    "\n",
    "# This function is same as baove but lot better without global variables\n",
    "def get_batch1(data, context_len=3, batch_size=1):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    #data = train if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_len, (batch_size,)) # get a random value\n",
    "    x = torch.stack([data[i:i+context_len] for i in ix]) # the first block size (context)\n",
    "    y = torch.stack([data[i+1:i+context_len+1] for i in ix]) # the target\n",
    "    return x, y\n",
    "\n",
    "torch.manual_seed(1)\n",
    "get_batch('train'), \"-----\", torch.manual_seed(1), get_batch1(train, 10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "#Some quick test to see what is what => learning moment Not needed\n",
    "print(torch.randint(256, (4,)), \" <== Prints random integet array of legth (4,) \" )\n",
    "\n",
    "# What does stack do?\n",
    "e = torch.tensor([0,1,2,3])\n",
    "torch.stack([e,e,e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size) \n",
    "        # We will create a embedding for tokens and \n",
    "        # for now kee pthe embedding length to be same as vocab_size \n",
    "        # Reminder: vocab size computed above was 64, remember?\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # (B,T,C) (batch size, window length (or context len), encoding len) tensor (4,8,vocab_size)\n",
    "        logits = self.token_embedding(idx) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshapes the logits tensor into a 2D tensor (flatten it)\n",
    "            targets = targets.view(B*T) # flatten the target tensor\n",
    "            loss = F.cross_entropy(logits, targets) #calculate the loss between the 2, measures how well the logits match the targets\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # this is used to generate new sequences of tokens\n",
    "        # also note that idx is (batch size, block size)\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "\n",
    "                logits, loss = self(idx, None) # obtain the predictions for the given input sequence (idx)\n",
    "\n",
    "                logits = logits[:, -1, :] # this becomes (batch size, channels), focus on the last step\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1) # convert into probabilities\n",
    "\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # give us 1 sample (1 prediction)\n",
    "\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # whatever the prediction is, concatenate it with the current idx and use this to predict the next element\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(65, 3)\n",
    "x,y = get_batch1(train, 3, 2)\n",
    "e, x, e(x).shape, e(x), e(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e(x), e(x).view(6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(2,3)\n",
    "i = torch.randn(3,2)\n",
    "print(f\"{m}\\n{i}\\n{m(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tril(torch.zeros(4, 4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
