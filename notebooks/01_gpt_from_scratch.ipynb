{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT From scratch\n",
    "\n",
    "Look upand read paper on Attention is all you need.\n",
    "https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "## Acknowledgements:\n",
    "The contents have been organized from these references:\n",
    "\n",
    "1. evshahs.medium.com/build-gpt-with-me-implementing-gpt-from-scratch-step-by-step-b2efe4e2f7e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuzzPHd6gIUQ",
    "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"Comment this line to run this cell\"\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTConfig:\n",
    "    attn_dropout = 0.1\n",
    "    embed_dropout = 0.1\n",
    "    ff_dropout = 0.1\n",
    "    \n",
    "    def __init__( self, vocab_size, max_len, **kwargs ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    num_heads = 12\n",
    "    num_blocks = 12\n",
    "    embed_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 239, 'max_len': 34}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.max_len = config.max_len\n",
    "        self.tok_embed = nn.Embedding(\n",
    "            config.vocab_size, embed_dim\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, config.max_len, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.embed_dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        # batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
    "        \n",
    "        tok_embedding = self.tok_embed(x)\n",
    "        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n",
    "        pos_embedding = self.pos_embed[:, :seq_len, :]\n",
    "        # pos_embedding.shape == (1, seq_len, embed_dim)\n",
    "        x = self.dropout(tok_embedding + pos_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        # x.shape == (batch_size, seq_len, vocab_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.attn_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
