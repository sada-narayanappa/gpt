{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuzzPHd6gIUQ",
    "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "#%%writefile \"../openaigpt.py\"\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/.django\") )\n",
    "import my_config\n",
    "from my_config import *\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  organization='org-IuMjyWnHvrzUr9jsi0dT6QmL',\n",
    "  project='proj_5AGDENKBvMR6sXoxezxXaWUY',\n",
    "  api_key=my_config.OPENAPI_GEO_KEY\n",
    ")\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    # Write your prompt\n",
    "    prompt=\"____\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 22:57:27,155 geoapp INFO: Prepping: 2024-08-11 22:57:27.155247\n",
      "2024-08-11 22:57:31,136 httpx INFO: HTTP Request: POST http://10.0.0.29:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I apologize for any confusion, but there seems to be a misunderstanding in your question. As of now, there isn\\'t a publicly known Chief Data Scientist for \"Martian.\" Martian could refer to various things, such as a company, a research organization, or even Mars the planet. To provide an accurate answer, more context is required.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile \"../olgpt.py\"\n",
    "import ollama, torch, logging,datetime\n",
    "from ollama import Client\n",
    "from mangorest.mango import webapi\n",
    "sys.path.append(os.path.expanduser(\"~/.django\") )\n",
    "    \n",
    "logger = logging.getLogger( \"geoapp\" )\n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available() ):\n",
    "    device = \"cuda\"\n",
    "\n",
    "\n",
    "OLLAMA_HOST= 'http://10.0.0.29:11434'\n",
    "OLLAMA_HOST= 'http://67.162.141.146:11434'\n",
    "OLLAMA_HOST=None\n",
    "if (os.path.exists(os.path.expanduser(\"~/.django/my_config.py\"))):\n",
    "    import my_config\n",
    "    from my_config import OLLAMA_HOST\n",
    "\n",
    "OLLAMA = Client(host=OLLAMA_HOST)\n",
    "#OLLAMA = Client()\n",
    "#--------------------------------------------------------------------------------------------------------    \n",
    "@webapi(\"/ollama/generate/\")\n",
    "def ollma_generate(request=None, model=\"mistral\", prompt=\"\", stream=True,**kwargs):\n",
    "    logger.info(f\"Prepping: {datetime.datetime.now()}\")\n",
    "    response = OLLAMA.generate(model=model, prompt=prompt, stream=stream)\n",
    "    # Stream response\n",
    "    ret = \"\"\n",
    "    for chunk in response:\n",
    "        data = chunk[\"response\"]\n",
    "        ret += data\n",
    "        \n",
    "    logger.debug(ret)\n",
    "    return ret\n",
    "\n",
    "ollma_generate(prompt=\"whos is the chief data scientist of  Martian\", stream=True)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
