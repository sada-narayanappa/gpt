{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import IPython, IPython.display, os, datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "print(f\"Tensorflow Version {tf.__version__}, Keras Vesion: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPTJForCausalLM, AutoTokenizer\n",
    "\n",
    "#model     = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model=TFGPTJForCausalLM.from_pretrained(\"TFgpt-j-6B\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"TFgpt-j-6B\")\n",
    "#model.save(\"TFgpt-j-6B-hd5\")\n",
    "\n",
    "#mtf = tf.keras.models.load_model(\"TFgpt-j-6B-hd5\")\n",
    "#mtf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt ='''In this white paper on T-TAURI from Lockheed Martin, we will discuss the following topics:'''\n",
    "tokens = tokenizer(prompt, return_tensors=\"tf\")\n",
    "inp_ids= tokens['input_ids']\n",
    "\n",
    "out_ids= model.generate(inp_ids, do_sample=True, temperature=0.9, max_length=128)\n",
    "otokens = tokenizer.batch_decode(out_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next = tokenizer.batch_encode_plus([gen_text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "gen_tokens1 = model.generate(next, do_sample=True, temperature=0.9, max_length=100,)\n",
    "\n",
    "gen_text1 = tokenizer.batch_decode(gen_tokens1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().get(\"gpt_j_generator\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snarayan/venv/py39/lib/python3.8/site-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/snarayan/venv/py39/lib/python3.8/site-packages/transformers/generation/tf_utils.py:702: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be honest, robots will eventually replace all but your most elite, indispensable laborers.\n",
      "\n",
      "But don't worry, the other 99.99...% of us will continue to live comfortably.\n",
      "\n",
      "The rise of robotics and AI will have long\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# generate sentences with TOP-K sampling\n",
    "from transformers import pipeline\n",
    "\n",
    "if ( globals().get(\"gpt_j_generator\") and globals().get(\"gpt_j_generator\") is None):\n",
    "    print(\"loading ...  \", end = \"\")\n",
    "    t1 = datetime.datetime.now()\n",
    "    gpt_j_generator = pipeline('text-generation', model='EleutherAI/gpt-j-6B')\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(f\" done in {t2-t1} seconds\")\n",
    "\n",
    "\n",
    "sentences = gpt_j_generator(\"To be honest, robots will\", do_sample=True, top_k=50, temperature=0.6, max_length=128, num_return_sequences=3)\n",
    "for s in sentences:\n",
    "    print(s[\"generated_text\"])\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be honest, robots will rise of robotics and AI will have long been in the future.\n",
      "\n",
      "In the past, it was only the military and the scientific community who could afford to have robots.\n",
      "\n",
      "But now, we are living in an era of the robots.\n",
      "\n",
      "Today, we can buy a robot vacuum cleaner that can clean our house, and can even take care of our pets.\n",
      "\n",
      "All of this is just the beginning.\n",
      "\n",
      "Soon, we will have robots that can do almost anything.\n",
      "\n",
      "With the development of AI, the robots will be able to think and act.\n",
      "\n",
      "And we\n",
      "==================================================\n",
      "To be honest, robots will rise of robotics and AI will have long been a reality. But, the implementation of robots in daily life may be a decade or more away.\n",
      "\n",
      "But, there are some robots which have been developed and are ready for use.\n",
      "\n",
      "1. Robotic surgery\n",
      "\n",
      "Robotic surgery is the use of robots to remotely perform surgery.\n",
      "\n",
      "This technology has been around for a while and the idea has been around for a while.\n",
      "\n",
      "But, it still hasn’t been widely implemented.\n",
      "\n",
      "If we look at the history of robotic surgery, it has been used in the military\n",
      "==================================================\n",
      "To be honest, robots will rise of robotics and AI will have long-term negative effects on employment, but they will also create a lot of new jobs.\n",
      "\n",
      "The good news is that the jobs that are created will be jobs that we don’t have today. Robots will take over the jobs that we have today. That’s a good thing.\n",
      "\n",
      "Robots will take over the jobs that we have today, but they will also create a lot of new jobs.\n",
      "\n",
      "The bad news is that the jobs that are created will be jobs that we don’t have today. Robots will take over\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = gpt_j_generator(\"To be honest, robots will rise of robotics and AI will have long\", do_sample=True, top_k=50, temperature=0.6, max_length=128, num_return_sequences=3)\n",
    "for s in sentences:\n",
    "    print(s[\"generated_text\"])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import tensorflow\n",
      "# 4 layer CNN with a softmax output\n",
      "# test on MNIST data set\n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "# \n",
      "#\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "prompt='''\n",
    "import tensorflow\n",
    "# 4 layer CNN with a softmax output\n",
    "# test on MNIST data set\n",
    "# '''\n",
    "\n",
    "sentences = gpt_j_generator(prompt, do_sample=False, top_k=40, temperature=0.7, max_length=256, num_return_sequences=1)\n",
    "for s in sentences:\n",
    "    print(s[\"generated_text\"])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\nimport tensorflow\\n# 4 layer CNN with a softmax output\\n# test on MNIST data set\\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n# \\n#'}\n"
     ]
    }
   ],
   "source": [
    "print(s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
